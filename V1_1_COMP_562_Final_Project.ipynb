{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "V1.1_COMP_562_Final_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNOSis61vYUf9kYZ+vgVOPA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlu51412/COMP562FinalProj/blob/main/V1_1_COMP_562_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeX8lNGEMU9p"
      },
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcRjDWYFP-fl"
      },
      "source": [
        "import os\n",
        "os.mkdir(\"generators\")\n",
        "os.mkdir(\"discriminators\")\n",
        "os.mkdir(\"train_plots\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKreZxjxk2BS"
      },
      "source": [
        "# Create Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ0Kr4xmNwcU"
      },
      "source": [
        "# WE could seperate the class and gan discriminator into two different things,\n",
        "# where they both recieve the generated output individually\n",
        "\n",
        "#  WE CAN PROBABLY PUT DROPOUT LAYERS TO HELP DECREASE OVERTRAINING\n",
        "learning_step = .0001\n",
        "\n",
        "def create_gan_discriminator(image_shape):\n",
        "  # gets convolved image from (base) and try to classify if this is a real or fake image\n",
        "  disc_input = tf.keras.Input(shape=image_shape)\n",
        "  disc_x = tf.keras.layers.Conv2D(64, (3,3),\n",
        "                                  activation=tf.keras.layers.LeakyReLU(.2),\n",
        "                                  padding='same')(disc_input)\n",
        "  disc_x = tf.keras.layers.Conv2D(128, (3,3), strides=(2,2), \n",
        "                                  activation=tf.keras.layers.LeakyReLU(.2),\n",
        "                                  padding='same')(disc_x)\n",
        "  disc_x = tf.keras.layers.Conv2D(128, (3,3), strides=(2,2), \n",
        "                                  activation=tf.keras.layers.LeakyReLU(.2),\n",
        "                                  padding='same')(disc_x)\n",
        "  disc_x = tf.keras.layers.Conv2D(256, (3,3), strides=(2,2), \n",
        "                                  activation=tf.keras.layers.LeakyReLU(.2),\n",
        "                                  padding='same')(disc_x)\n",
        "  disc_x = tf.keras.layers.Flatten()(disc_x)\n",
        "  disc_x = tf.keras.layers.Dropout(.4)(disc_x)\n",
        "  disc_x = tf.keras.layers.Dense(1, activation='sigmoid')(disc_x)\n",
        "  full_discriminator = tf.keras.Model(inputs=disc_input, outputs=disc_x)\n",
        "  tf.keras.utils.plot_model(full_discriminator, \"gan_discriminator.png\", show_shapes=True)\n",
        "  optimizerfn = tf.keras.optimizers.Adam(learning_rate=learning_step, beta_1=.9)\n",
        "  full_discriminator.compile(loss=tf.keras.losses.BinaryCrossentropy(), \n",
        "                        optimizer=optimizerfn, metrics=['accuracy'])\n",
        "  return full_discriminator\n",
        "\n",
        "def create_generator(input_dim):\n",
        "  # maybe we can input the generated image into itself\n",
        "  # note: we will pass random noise to this generator\n",
        "  # (in hopes it will generate the correct variations of an object)\n",
        "  # two inputs: random noise and the given image class we want to generate\n",
        "  gen_x_input = tf.keras.Input(shape=(input_dim))\n",
        "  # 4x4 upscaling foundation\n",
        "  num_nodes = 4*4*256 \n",
        "  gen_x = tf.keras.layers.Dense(num_nodes)(gen_x_input)\n",
        "  gen_x = tf.keras.layers.Reshape((4, 4, 256))(gen_x)\n",
        "  # upsize\n",
        "  # 8x8\n",
        "  gen_x = tf.keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2),  \n",
        "                                          activation=tf.keras.layers.LeakyReLU(.2),\n",
        "                                          padding=\"same\")(gen_x)\n",
        "\n",
        "  # 16x16\n",
        "  gen_x = tf.keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2),  \n",
        "                                          activation=tf.keras.layers.LeakyReLU(.2),\n",
        "                                          padding=\"same\")(gen_x)\n",
        "  # 32x32\n",
        "  gen_x = tf.keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2),  \n",
        "                                          activation=tf.keras.layers.LeakyReLU(.2),\n",
        "                                          padding=\"same\")(gen_x)\n",
        "  # 28x28\n",
        "  gen_x = tf.keras.layers.Conv2D(1, (5,5), activation='tanh')(gen_x)\n",
        "  generator = tf.keras.Model(inputs=gen_x_input, outputs=gen_x)\n",
        "  tf.keras.utils.plot_model(generator, \"generator.png\", show_shapes=True)\n",
        "  return generator\n",
        "\n",
        "\n",
        "def create_gan(generator, gan_discriminator, input_dim):\n",
        "  # will be same input as generator\n",
        "  gan_x_input = tf.keras.Input(shape=(input_dim))\n",
        "  gan_x = generator(gan_x_input) \n",
        "\n",
        "  # we want to train the gan to match the classifier and if it's real or fake\n",
        "  real_or_fake = gan_discriminator(gan_x)\n",
        "  gan_discriminator.trainable = False\n",
        "\n",
        "  gan = tf.keras.Model(inputs=gan_x_input, outputs=real_or_fake)\n",
        "  tf.keras.utils.plot_model(gan, \"gan.png\", show_shapes=True)\n",
        "  optimizerfn = tf.keras.optimizers.Adam(learning_rate=learning_step, beta_1=.9)\n",
        "  gan.compile(loss=tf.keras.losses.BinaryCrossentropy(), \n",
        "                        optimizer=optimizerfn, metrics=['accuracy'])\n",
        "  \n",
        "  return gan\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfDEhO8Lk4bc"
      },
      "source": [
        "# Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyhFnl4YQdpN"
      },
      "source": [
        "# important functions for training\n",
        "from matplotlib import pyplot\n",
        "import shutil\n",
        "\n",
        "def generate_real_samples(dataset, num_samples, labels, class_num):\n",
        "  \n",
        "  class_data = dataset[labels==class_num]\n",
        "  # get random \"real\" samples\n",
        "  rand_idx = np.random.randint(0, class_data.shape[0], num_samples)\n",
        "  real_im = class_data[rand_idx]\n",
        "  # real class label of \"1\"\n",
        "  true_class = np.ones((num_samples, 1))  \n",
        "  return real_im, true_class\n",
        "\n",
        "def generate_fake_samples(generator, input_dim, num_samples):\n",
        "  # generate noise input for GAN with randomness and it's own prediction\n",
        "  inputs = np.random.randn(input_dim*num_samples)\n",
        "  # reshape into dimensions for the network\n",
        "  inputs = inputs.reshape(num_samples, input_dim)\n",
        "  fake_im = generator.predict(inputs)\n",
        "  true_class = np.zeros((num_samples, 1))\n",
        "  return fake_im, true_class\n",
        "\n",
        "def generate_GAN_samples(generator, input_dim, num_samples):\n",
        "  # generate noise input for GAN with randomness and it's own prediction\n",
        "  inputs = np.random.randn(input_dim*num_samples)\n",
        "  # reshape into dimensions for the network\n",
        "  inputs = inputs.reshape(num_samples, input_dim)\n",
        "  true_class = np.ones((num_samples, 1))\n",
        "  return inputs, true_class\n",
        "\n",
        "# eval current performance\n",
        "def summarize_performance(epoch, generator, gan_discriminator, dataset, input_dim, labels, class_names, class_num,\n",
        "                          num_samples=150):\n",
        "  # prepare real samples\n",
        "  X_real, real_gan = generate_real_samples(dataset, num_samples, labels, class_num)\n",
        "  # evaluate discriminator on real examples\n",
        "  real_eval = gan_discriminator.evaluate(X_real, real_gan, verbose=0)\n",
        "\n",
        "  # prepare fake examples\n",
        "  x_fake, gan_fake = generate_fake_samples(generator, input_dim, num_samples)\n",
        "  # evaluate discriminator on fake examples\n",
        "  fake_eval = gan_discriminator.evaluate(x_fake, gan_fake, verbose=0)\n",
        "  \n",
        "  # summarize discriminator performance\n",
        "  print(\"____________________\")\n",
        "  print(\"Epoch:\",epoch+1, \"|| Class:\", class_names[class_num])\n",
        "  print(\">real_eval:\", real_eval)\n",
        "  print(\">fake_eval:\", fake_eval)\n",
        "  print(\"____________________\")\n",
        "  # save plot\n",
        "  save_plot(x_fake, epoch, class_names, class_num, 4)\n",
        "  # save the generator model tile file\n",
        "  filename = 'generator_model_' + str(epoch+1) + '_' + class_names[class_num] + '.h5' \n",
        "  generator.save(filename)\n",
        "  shutil.move(filename, './generators/'+filename)\n",
        "\n",
        "  filename = 'gan_discriminator_model_' + str(epoch+1) + '_' + class_names[class_num] + '.h5' \n",
        "  gan_discriminator.save(filename)\n",
        "  shutil.move(filename, './discriminators/'+filename)\n",
        "\n",
        "# create and save a plot of generated images\n",
        "def save_plot(examples, epoch, class_names, class_num, n=4):\n",
        "  # scale from [-1,1] to [0,1]\n",
        "  examples = (examples + 1) / 2.0\n",
        "  # plot images\n",
        "  for i in range(n * n):\n",
        "    # define subplot\n",
        "    pyplot.subplot(n, n, 1 + i)\n",
        "    pyplot.title(class_names[class_num])\n",
        "    pyplot.tight_layout(pad=.5)\n",
        "    # turn off axis\n",
        "    pyplot.axis('off')\n",
        "    # plot raw pixel data\n",
        "    pyplot.imshow(examples[i,:,:,0], cmap=\"gray\")\n",
        "\n",
        "\t# save plot to file\n",
        "  filename = 'generated_plot_e' + str(epoch+1) + '_' + class_names[class_num] + '.png'\n",
        "  pyplot.savefig(filename)\n",
        "  pyplot.close()\n",
        "  shutil.move(filename, './train_plots/'+filename)\n",
        "  \n",
        "\n",
        "def show_rand_plot(generators, input_dim, num_samples, labels, class_names, n=4):\n",
        "  # prepare fake examples\n",
        "  x_fake, true_class = generate_fake_samples(generator, input_dim, num_samples, labels)\n",
        "  # scale from [-1,1] to [0,1]\n",
        "  x_fake = (x_fake + 1) / 2.0\n",
        "  # plot images\n",
        "  for i in range(n * n):\n",
        "    # define subplot\n",
        "    pyplot.subplot(n, n, 1 + i)\n",
        "    pyplot.title(class_names[np.where(real_label[i] == 1)[0][0]])\n",
        "    pyplot.tight_layout(pad=.5)\n",
        "    # turn off axis\n",
        "    pyplot.axis('off')\n",
        "    # plot raw pixel data\n",
        "    pyplot.imshow(x_fake[i,:,:,0], cmap=\"gray\")\n",
        "# train the generator and discriminator\n",
        "def train(generators, gan_discriminators, gans, class_discriminator_base, class_discriminator, cd_features,\n",
        "          dataset, labels, input_dim, sample_num, n_epochs=200, n_batch=128):\n",
        "          \n",
        "  batches_per_epoch = int(dataset.shape[0] / n_batch)\n",
        "  half_batch = int(n_batch / 2)\n",
        "  # manually enumerate epochs\n",
        "  for i in range(n_epochs):\n",
        "    # batch training (for 1 epoch)\n",
        "    for j in range(batches_per_epoch):\n",
        "      d_loss_gan_disc_real = 0\n",
        "      d_loss_gan_disc_fake = 0\n",
        "      g_loss = 0\n",
        "      \n",
        "      for k in range(len(generators)):\n",
        "        # get this classes generators and gan\n",
        "        gan_discriminator = gan_discriminators[k]\n",
        "        generator = generators[k]\n",
        "        gan = gans[k]\n",
        "        \n",
        "        real_im, real_gan = generate_real_samples(dataset, half_batch, labels, k)\n",
        "        d_loss_real_local, _ = gan_discriminator.train_on_batch(real_im, real_gan) # gan loss real\n",
        "        d_loss_gan_disc_real += d_loss_real_local\n",
        "        fake_im, fake_gan = generate_fake_samples(generator, input_dim, half_batch)\n",
        "        d_loss_fake_local, _ = gan_discriminator.train_on_batch(fake_im, fake_gan) # gan loss fake\n",
        "        d_loss_gan_disc_fake += d_loss_fake_local\n",
        "\n",
        "        gan_input, inverted_label = generate_GAN_samples(generator, input_dim, half_batch) # returns with random noise for gan input\n",
        "        g_loss_local, _ = gan.train_on_batch(gan_input, inverted_label) # train generator with inverted labels via gan\n",
        "        g_loss += g_loss_local\n",
        "      # summarize loss on this batch\n",
        "      if (j+1) % n_batch == 0:\n",
        "        print(\"Epoch:\", i+1, \" - Batch:\",str(j+1)+\"/\"+str(batches_per_epoch),\n",
        "              \"avg_d_loss_gan (real/fake):\" + str(d_loss_gan_disc_real/len(labels)) + \",\" + str(d_loss_gan_disc_fake/len(labels)),\n",
        "              \"avg_gan_loss:\" + str(g_loss/len(labels)))\n",
        "    # evaluate the model performance, sometimes\n",
        "    if (i+1) % 10 == 0:\n",
        "      for k in range(len(generators)):\n",
        "        summarize_performance(i, generators[k], gan_discriminators[k], dataset, input_dim, labels, class_names, k, sample_num)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35jPqfM7k6S-"
      },
      "source": [
        "# Init and Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12H6tp3NDcQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e3798a7-5cdf-4a2d-8921-54a2a0a18ddd"
      },
      "source": [
        "# initializations before training\n",
        "# load dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "class_names = ['zero', 'one', 'two', 'three', 'four',\n",
        "               'five', 'six', 'seven', 'eight', 'nine']\n",
        "# for tensorflow format (they assume 3rd dimension with color channels)\n",
        "train_images = np.expand_dims(train_images, 3)\n",
        "train_images = np.interp(train_images, (train_images.min(), train_images.max()), (-1, +1))\n",
        "\n",
        "test_images = np.expand_dims(test_images, 3)\n",
        "test_images = np.interp(test_images, (test_images.min(), test_images.max()), (-1, +1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyywK4qm3g80"
      },
      "source": [
        "im_shape = train_images[0].shape  # is the noise dim\n",
        "sample_num = 16 # make this a full square\n",
        "# length of noise vector for random generation\n",
        "label_shape = len(class_names)\n",
        "generator_input_dim = 100\n",
        "# init the generator array\n",
        "generators = []\n",
        "gans = []\n",
        "gan_discriminators = []\n",
        "\n",
        "# create the generator array\n",
        "for i in range(label_shape):\n",
        "  generators.append(create_generator(generator_input_dim))\n",
        "  gan_discriminators.append(create_gan_discriminator(im_shape))\n",
        "  gans.append(create_gan(generators[i], gan_discriminators[i], generator_input_dim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgHPtTZcf5QM"
      },
      "source": [
        "train(generators, gan_discriminators, gans, class_discriminator_base, class_discriminator,  cd_features,\n",
        "      train_images, train_labels, generator_input_dim, sample_num, n_epochs=1000, \n",
        "      n_batch=128)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}